{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a18ed0",
   "metadata": {},
   "source": [
    "# Arrow Judge Notebook\n",
    "\n",
    "Evaluates how similar reapplied arrows are to original arrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67316538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949dbee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o\"  # Advanced multimodal model\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set in your environment.\")\n",
    "\n",
    "client = OpenAI(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b616a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_data_url(path: str) -> str:\n",
    "    ext = Path(path).suffix.lower().replace(\".\", \"\")\n",
    "    if ext == \"jpg\":\n",
    "        ext = \"jpeg\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    return f\"data:image/{ext};base64,{data}\"\n",
    "\n",
    "def build_rubric_prompt() -> str:\n",
    "    return \"\"\"You are an expert annotator for arrow segmentation and overlay quality.\n",
    "You will recieve an ORIGINAL image and a REAPPLIED image. The original image will be a medical image with overlaid arrows.\n",
    "The reapplied image will have additional arrows that have been lifted off of the original image with segmentation and re-overlayed at random locations on the image.\n",
    "Your job is to determine how natural and inconspicuous the new arrows look compared to the original arrows.\n",
    "\n",
    "Judge similarity of arrow placement and appearance using this rubric:\n",
    "\n",
    "1) Arrow General Shape (1-5)\n",
    "   - 5: Shape of new arrows matches the shape of the original arrows almost exactly\n",
    "   - 4: Shape of new arrows is close to the original arrows but still distinguishable due to very minor imperfections\n",
    "   - 3: New arrows are clearly distinguishable based on shape imperfections\n",
    "   - 2: New arrows are barely reminiscent of original arrow \n",
    "   - 1: Shape is not at all reminiscent of original arrow\n",
    "\n",
    "5) Arrow Style (1-5)\n",
    "   - 5: Style and color almost identical to original arrows\n",
    "   - 4: Style and color match well\n",
    "   - 3: Noticeable differences in style and color\n",
    "   - 2: Major differences in style and color\n",
    "   - 1: Style and color are completely different\n",
    "\n",
    "6) Arrow Edges (1-5)\n",
    "   - 5: Edges of new arrows are crisp, clean, and straight\n",
    "   - 4: Edges of new arrows are clear and straight, with minor speckle or jaggedness\n",
    "   - 3: Edges of new arrows make them look clearly segmented and different\n",
    "   - 2: Edges very jagged or have a lot of speckle. Not clean lines.\n",
    "   - 1: Cannot tell where the edges of the arrow are.\n",
    "\n",
    "Output JSON only with:\n",
    "{\n",
    "  \"overall_score\": 3-15,\n",
    "  \"per_metric\": {\n",
    "    \"shape\": 0-5,\n",
    "    \"style\": 0-5,\n",
    "    \"edges\": 0-5\n",
    "  },\n",
    "  \"notes\": \"justification for scores\"\n",
    "}\n",
    "\n",
    "Compute overall_score as a sum:\n",
    "shape + style + edges\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9070616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_arrow_similarity(reapplied_path: str, original_path: str) -> Dict[str, Any]:\n",
    "    reapplied_url = image_to_data_url(reapplied_path)\n",
    "    original_url = image_to_data_url(original_path)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a precise visual evaluator. Follow the rubric exactly.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": build_rubric_prompt()},\n",
    "                {\"type\": \"text\", \"text\": \"ORIGINAL IMAGE:\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": original_url}},\n",
    "                {\"type\": \"text\", \"text\": \"REAPPLIED IMAGE:\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": reapplied_url}},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    return resp.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac8e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "REAPPLIED_PATH = \"reapplied/pic5.jpeg\"\n",
    "ORIGINAL_PATH = \"../pics/pic5.jpeg\"\n",
    "\n",
    "result_json = judge_arrow_similarity(REAPPLIED_PATH, ORIGINAL_PATH)\n",
    "print(result_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raivn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
